"""
Agente Aut√¥nomo para An√°lise de CSV - Vers√£o H√≠brida
Ollama (local) + OpenAI (fallback) para reasoning completo
I2A2 Challenge - Setembro 2025
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import json
import os
from datetime import datetime
import requests
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from scipy import stats
import io
import base64

# Adicionar OpenAI
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("‚ö†Ô∏è OpenAI n√£o instalado. Execute: pip install openai")

warnings.filterwarnings('ignore')

@dataclass
class DataAnalysis:
    """Estrutura para armazenar resultados de an√°lise"""
    summary_stats: Dict
    data_types: Dict
    missing_values: Dict
    correlations: pd.DataFrame
    outliers: Dict
    fraud_indicators: Optional[Dict] = None

class CSVMemory:
    """Sistema de mem√≥ria para o agente"""
    def __init__(self):
        self.conversation_history = []
        self.analysis_cache = {}
        self.insights = []
        self.current_dataset_info = None
    
    def add_conversation(self, question: str, answer: str, analysis_type: str = "general"):
        """Adiciona intera√ß√£o √† mem√≥ria"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "answer": answer,
            "analysis_type": analysis_type
        }
        self.conversation_history.append(entry)
    
    def add_insight(self, insight: str, category: str = "general"):
        """Adiciona insight descoberto"""
        self.insights.append({
            "timestamp": datetime.now().isoformat(),
            "insight": insight,
            "category": category
        })
    
    def get_context(self) -> str:
        """Retorna contexto relevante para o LLM"""
        context = f"Dataset atual: {self.current_dataset_info}\n"
        context += f"Insights descobertos: {len(self.insights)}\n"
        
        # √öltimas 3 intera√ß√µes para contexto
        recent_conv = self.conversation_history[-3:] if self.conversation_history else []
        for conv in recent_conv:
            context += f"Q: {conv['question'][:100]}...\nA: {conv['answer'][:150]}...\n"
        
        return context

class HybridReasoning:
    """Sistema de reasoning h√≠brido: Ollama + OpenAI"""
    
    def __init__(self, ollama_model: str = "llama3.1", openai_api_key: str = None):
        self.ollama_model = ollama_model
        self.ollama_url = "http://localhost:11434/api/generate"
        self.openai_client = None
        
        # Configurar OpenAI se dispon√≠vel
        if OPENAI_AVAILABLE and openai_api_key:
            self.openai_client = openai.OpenAI(api_key=openai_api_key)
        elif OPENAI_AVAILABLE:
            # Tentar usar vari√°vel de ambiente
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key:
                self.openai_client = openai.OpenAI(api_key=api_key)
    
    def check_ollama_status(self) -> bool:
        """Verifica se Ollama est√° dispon√≠vel"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=3)
            return response.status_code == 200
        except:
            return False
    
    def query_ollama(self, prompt: str, system_prompt: str = None, timeout: int = 15) -> Optional[str]:
        """Consulta Ollama com timeout reduzido"""
        try:
            if not self.check_ollama_status():
                return None
                
            full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
            
            payload = {
                "model": self.ollama_model,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "num_ctx": 2048  # Reduzido para melhor performance
                }
            }
            
            response = requests.post(self.ollama_url, json=payload, timeout=timeout)
            if response.status_code == 200:
                return response.json()["response"]
            else:
                print(f"‚ö†Ô∏è Ollama erro {response.status_code}")
                return None
                
        except Exception as e:
            print(f"‚ö†Ô∏è Ollama indispon√≠vel: {str(e)[:50]}...")
            return None
    
    def query_openai(self, prompt: str, system_prompt: str = None) -> Optional[str]:
        """Consulta OpenAI como fallback"""
        try:
            if not self.openai_client:
                return None
            
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                max_tokens=500,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"‚ö†Ô∏è OpenAI erro: {str(e)[:50]}...")
            return None
    
    def query_with_fallback(self, prompt: str, system_prompt: str = None) -> str:
        """Reasoning h√≠brido: Ollama primeiro, OpenAI como backup"""
        
        # Tentar Ollama primeiro (mais r√°pido e local)
        print("üîÑ Tentando Ollama...")
        ollama_response = self.query_ollama(prompt, system_prompt, timeout=10)
        
        if ollama_response and len(ollama_response.strip()) > 20:
            print("‚úÖ Resposta via Ollama")
            return f"üß† **An√°lise Local (Ollama):** {ollama_response}"
        
        # Fallback para OpenAI
        print("üîÑ Fallback para OpenAI...")
        openai_response = self.query_openai(prompt, system_prompt)
        
        if openai_response:
            print("‚úÖ Resposta via OpenAI")
            return f"ü§ñ **An√°lise IA (OpenAI):** {openai_response}"
        
        # Fallback final: reasoning program√°tico
        print("‚ö†Ô∏è Usando reasoning program√°tico")
        return self.programmatic_reasoning(prompt)
    
    def programmatic_reasoning(self, prompt: str, analysis_data: DataAnalysis = None) -> str:
        """Reasoning baseado em regras com dados reais"""
        prompt_lower = prompt.lower()
        
        if "outlier" in prompt_lower and analysis_data:
            outlier_info = []
            for col, data in analysis_data.outliers.items():
                outlier_info.append(f"{col}: {data['count']:,} ({data['percentage']:.1f}%)")
            
            if outlier_info:
                return f"""üìä **An√°lise de Outliers:** Detectados outliers em {len(outlier_info)} colunas:
                {' | '.join(outlier_info[:5])}
                
                As vari√°veis V1-V28 com outliers podem indicar padr√µes at√≠picos relevantes para detec√ß√£o de fraudes."""
        
        elif "correla√ß√£o" in prompt_lower and analysis_data and analysis_data.correlations is not None:
            corr_matrix = analysis_data.correlations
            max_corr = corr_matrix.abs().unstack().sort_values(ascending=False)
            max_corr = max_corr[max_corr < 1.0].head(3)
            
            return f"""üìà **An√°lise de Correla√ß√£o:** Principais correla√ß√µes encontradas:
            {' | '.join([f'{idx[0]}-{idx[1]}: {val:.3f}' for idx, val in max_corr.items()])}
            
            Baixas correla√ß√µes confirmam efic√°cia da transforma√ß√£o PCA para preservar privacidade."""
        
        elif "fraude" in prompt_lower and analysis_data and analysis_data.fraud_indicators:
            fraud_info = analysis_data.fraud_indicators
            return f"""üö® **An√°lise de Fraude:** 
            - Total de fraudes: {fraud_info['fraud_count']:,} casos
            - Taxa de fraude: {fraud_info['fraud_percentage']:.3f}%
            - Transa√ß√µes normais: {fraud_info['normal_count']:,}
            
            Taxa extremamente baixa indica dataset real t√≠pico de sistemas de pagamento."""
        
        elif "dados faltantes" in prompt_lower and analysis_data:
            missing_total = sum(analysis_data.missing_values.values())
            if missing_total == 0:
                return """‚úÖ **Qualidade dos Dados:** Zero valores faltantes em todas as colunas.
                Dataset de alta qualidade, pronto para an√°lises sem necessidade de imputa√ß√£o."""
            else:
                missing_cols = {k: v for k, v in analysis_data.missing_values.items() if v > 0}
                return f"""‚ö†Ô∏è **Dados Faltantes:** {missing_total:,} valores ausentes em {len(missing_cols)} colunas:
                {' | '.join([f'{k}: {v}' for k, v in list(missing_cols.items())[:3]])}"""
        
        elif "conclus" in prompt_lower or "insight" in prompt_lower:
            insights = []
            if analysis_data:
                if analysis_data.fraud_indicators:
                    rate = analysis_data.fraud_indicators['fraud_percentage']
                    insights.append(f"Taxa de fraude: {rate:.3f}% (t√≠pica de cen√°rios reais)")
                
                missing_total = sum(analysis_data.missing_values.values())
                if missing_total == 0:
                    insights.append("Dataset limpo sem valores faltantes")
                
                outlier_cols = len([col for col, data in analysis_data.outliers.items() if data['count'] > 0])
                insights.append(f"Outliers detectados em {outlier_cols} colunas")
            
            return f"""üìä **Conclus√µes T√©cnicas:**
            {' | '.join(insights)}
            
            Recomenda√ß√µes: Usar t√©cnicas de balanceamento para ML, focar em detec√ß√£o de anomalias, 
            monitorar vari√°veis com outliers para identifica√ß√£o de padr√µes fraudulentos."""
        
        else:
            return """üîç **An√°lise Autom√°tica:** Dataset analisado com sucesso. 
            Para insights espec√≠ficos, pergunte sobre outliers, correla√ß√µes, fraudes ou conclus√µes."""

class CSVAgentHybrid:
    """Agente principal com reasoning h√≠brido"""
    
    def __init__(self, ollama_model: str = "llama3.1", openai_api_key: str = None):
        self.memory = CSVMemory()
        self.reasoning = HybridReasoning(ollama_model, openai_api_key)
        self.current_df = None
        self.current_analysis = None
        
        print("ü§ñ Agente CSV H√≠brido inicializado")
        print(f"üì° Ollama: {'‚úÖ' if self.reasoning.check_ollama_status() else '‚ùå'}")
        print(f"üåê OpenAI: {'‚úÖ' if self.reasoning.openai_client else '‚ùå'}")
        
    def load_csv(self, file_path: str) -> bool:
        """Carrega arquivo CSV"""
        try:
            print(f"üîÑ Carregando CSV: {file_path}")
            
            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
            
            for encoding in encodings:
                try:
                    self.current_df = pd.read_csv(file_path, encoding=encoding)
                    print(f"‚úÖ CSV carregado com encoding {encoding}")
                    print(f"üìä Shape: {self.current_df.shape}")
                    break
                except UnicodeDecodeError:
                    continue
            
            if self.current_df is None:
                print("‚ùå Erro: N√£o foi poss√≠vel carregar o CSV")
                return False
            
            # Armazenar info na mem√≥ria
            self.memory.current_dataset_info = {
                "filename": os.path.basename(file_path),
                "shape": self.current_df.shape,
                "columns": list(self.current_df.columns),
                "loaded_at": datetime.now().isoformat()
            }
            
            # Insight autom√°tico
            self.memory.add_insight(
                f"Dataset carregado: {self.current_df.shape[0]:,} linhas, {self.current_df.shape[1]} colunas",
                "data_loading"
            )
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao carregar CSV: {str(e)}")
            return False
    
    def perform_eda(self) -> DataAnalysis:
        """Realiza an√°lise explorat√≥ria completa"""
        if self.current_df is None:
            raise ValueError("Nenhum dataset carregado")
        
        print("üîç Realizando an√°lise explorat√≥ria...")
        df = self.current_df
        
        # Estat√≠sticas b√°sicas
        summary_stats = {}
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            summary_stats[col] = {
                'mean': float(df[col].mean()),
                'median': float(df[col].median()),
                'std': float(df[col].std()),
                'min': float(df[col].min()),
                'max': float(df[col].max()),
                'q25': float(df[col].quantile(0.25)),
                'q75': float(df[col].quantile(0.75))
            }
        
        # Tipos de dados
        data_types = {str(k): str(v) for k, v in df.dtypes.to_dict().items()}
        
        # Valores ausentes
        missing_values = {str(k): int(v) for k, v in df.isnull().sum().to_dict().items()}
        
        # Correla√ß√µes (limitadas para performance)
        correlations = None
        if len(numeric_cols) > 1:
            corr_cols = numeric_cols[:10]
            correlations = df[corr_cols].corr()
        
        # Detec√ß√£o de outliers
        outliers = {}
        for col in numeric_cols[:5]:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)
            outliers[col] = {
                'count': int(outlier_mask.sum()),
                'percentage': float((outlier_mask.sum() / len(df)) * 100),
                'bounds': [float(lower_bound), float(upper_bound)]
            }
        
        # An√°lise espec√≠fica de fraude
        fraud_indicators = None
        if 'Class' in df.columns:
            fraud_count = int(df['Class'].sum())
            total_count = len(df)
            fraud_indicators = {
                'fraud_count': fraud_count,
                'fraud_percentage': float((fraud_count / total_count) * 100),
                'normal_count': total_count - fraud_count,
                'class_distribution': {str(k): int(v) for k, v in df['Class'].value_counts().to_dict().items()}
            }
            
            self.memory.add_insight(
                f"Taxa de fraude: {fraud_indicators['fraud_percentage']:.2f}%",
                "fraud_detection"
            )
        
        self.current_analysis = DataAnalysis(
            summary_stats=summary_stats,
            data_types=data_types,
            missing_values=missing_values,
            correlations=correlations,
            outliers=outliers,
            fraud_indicators=fraud_indicators
        )
        
        print("‚úÖ An√°lise explorat√≥ria conclu√≠da")
        return self.current_analysis
    
    def generate_visualization(self, viz_type: str, columns: List[str] = None) -> str:
        """Gera visualiza√ß√µes dos dados"""
        if self.current_df is None:
            return "Erro: Nenhum dataset carregado"
        
        df = self.current_df
        plt.style.use('default')
        
        try:
            if viz_type == "correlation_heatmap":
                numeric_df = df.select_dtypes(include=[np.number])
                if len(numeric_df.columns) > 1:
                    cols_to_plot = numeric_df.columns[:15]
                    
                    plt.figure(figsize=(12, 10))
                    correlation_matrix = numeric_df[cols_to_plot].corr()
                    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
                              fmt='.2f', square=True, cbar_kws={'label': 'Correla√ß√£o'})
                    plt.title('Matriz de Correla√ß√£o - Agente CSV I2A2', fontsize=16, fontweight='bold')
                    plt.tight_layout()
                    
                    img_buffer = io.BytesIO()
                    plt.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')
                    img_buffer.seek(0)
                    img_str = base64.b64encode(img_buffer.read()).decode()
                    plt.close()
                    
                    return f"data:image/png;base64,{img_str}"
                else:
                    return "Erro: N√£o h√° colunas num√©ricas suficientes"
            
            elif viz_type == "fraud_analysis" and 'Class' in df.columns:
                fig, axes = plt.subplots(2, 2, figsize=(15, 12))
                
                # Distribui√ß√£o de classes
                class_counts = df['Class'].value_counts()
                axes[0,0].bar(['Normal', 'Fraude'], [class_counts[0], class_counts[1]], 
                             color=['skyblue', 'salmon'])
                axes[0,0].set_title('Distribui√ß√£o: Normal vs Fraude', fontweight='bold')
                axes[0,0].set_ylabel('Quantidade')
                
                for i, v in enumerate([class_counts[0], class_counts[1]]):
                    axes[0,0].text(i, v + max(class_counts) * 0.01, f'{v:,}', 
                                  ha='center', va='bottom', fontweight='bold')
                
                # Amount por classe (se existir)
                if 'Amount' in df.columns:
                    df.boxplot(column='Amount', by='Class', ax=axes[0,1])
                    axes[0,1].set_title('Distribui√ß√£o de Valores por Classe')
                
                # Distribui√ß√£o temporal
                if 'Time' in df.columns:
                    normal_time = df[df['Class']==0]['Time']
                    fraud_time = df[df['Class']==1]['Time']
                    
                    axes[1,0].hist([normal_time, fraud_time], bins=50, alpha=0.7, 
                                  label=['Normal', 'Fraude'], color=['skyblue', 'salmon'])
                    axes[1,0].set_title('Distribui√ß√£o Temporal')
                    axes[1,0].legend()
                
                # Correla√ß√µes com fraude
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                correlations_with_class = df[numeric_cols].corrwith(df['Class']).sort_values(key=abs, ascending=False)
                top_corr = correlations_with_class.head(10)
                
                colors = ['red' if x > 0 else 'blue' for x in top_corr.values]
                axes[1,1].barh(range(len(top_corr)), top_corr.values, color=colors, alpha=0.7)
                axes[1,1].set_yticks(range(len(top_corr)))
                axes[1,1].set_yticklabels(top_corr.index)
                axes[1,1].set_title('Top 10 Correla√ß√µes com Fraude')
                
                plt.suptitle('An√°lise de Fraudes - Agente CSV I2A2', fontsize=16, fontweight='bold')
                plt.tight_layout()
                
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')
                img_buffer.seek(0)
                img_str = base64.b64encode(img_buffer.read()).decode()
                plt.close()
                
                return f"data:image/png;base64,{img_str}"
                    
        except Exception as e:
            plt.close('all')
            return f"Erro ao gerar visualiza√ß√£o: {str(e)}"
        
        return "Tipo de visualiza√ß√£o n√£o reconhecido"
    
    def answer_question(self, question: str) -> Dict[str, Any]:
        """Responde pergunta com reasoning h√≠brido"""
        if self.current_df is None:
            return {"error": "Nenhum dataset carregado"}
        
        print(f"ü§î Processando: {question[:50]}...")
        
        # EDA se necess√°rio
        if self.current_analysis is None:
            self.perform_eda()
        
        # Preparar contexto para IA
        memory_context = self.memory.get_context()
        
        system_prompt = """Voc√™ √© um especialista em an√°lise de dados e detec√ß√£o de fraudes. 
        Analise os dados fornecidos e responda de forma t√©cnica, clara e pr√°tica. 
        Forne√ßa insights espec√≠ficos e recomenda√ß√µes quando apropriado."""
        
        # Preparar dados para a IA
        data_context = f"""
        PERGUNTA: {question}
        
        DADOS DO DATASET:
        - Linhas: {self.current_df.shape[0]:,}
        - Colunas: {self.current_df.shape[1]}
        - Tipos: {self.current_analysis.data_types}
        - Dados faltantes: {self.current_analysis.missing_values}
        - Fraude info: {self.current_analysis.fraud_indicators}
        - Outliers: {self.current_analysis.outliers}
        
        CONTEXTO: {memory_context[:300]}
        
        Responda de forma espec√≠fica e t√©cnica.
        """
        
        # Usar reasoning h√≠brido
        ai_response = self.reasoning.query_with_fallback(data_context, system_prompt)
        
        # Verificar se precisa de visualiza√ß√£o
        viz_keywords = ["gr√°fico", "visualiza√ß√£o", "chart", "plot", "distribui√ß√£o", "correla√ß√£o", "mostrar", "visual"]
        needs_viz = any(keyword in question.lower() for keyword in viz_keywords)
        
        response = {
            "answer": ai_response,
            "visualization": None,
            "data_insights": []
        }
        
        # Gerar visualiza√ß√£o se necess√°rio
        if needs_viz:
            if "correla√ß√£o" in question.lower():
                print("üìä Gerando matriz de correla√ß√£o...")
                viz_data = self.generate_visualization("correlation_heatmap")
                if viz_data.startswith("data:image"):
                    response["visualization"] = viz_data
            elif any(word in question.lower() for word in ["fraude", "fraud", "class"]) and 'Class' in self.current_df.columns:
                print("üö® Gerando an√°lise de fraude...")
                viz_data = self.generate_visualization("fraud_analysis")
                if viz_data.startswith("data:image"):
                    response["visualization"] = viz_data
        
        # Insights autom√°ticos
        if self.current_analysis.fraud_indicators:
            fraud_rate = self.current_analysis.fraud_indicators['fraud_percentage']
            response["data_insights"].append(f"Taxa de fraude: {fraud_rate:.2f}%")
            
            if fraud_rate > 1:
                response["data_insights"].append("‚ö†Ô∏è Alta taxa de fraude")
            elif fraud_rate < 0.5:
                response["data_insights"].append("‚úÖ Dataset t√≠pico de fraudes (baixa taxa)")
        
        # Adicionar √† mem√≥ria
        self.memory.add_conversation(question, ai_response)
        
        print("‚úÖ Resposta gerada com reasoning h√≠brido")
        return response
    
    def get_conclusions(self) -> str:
        """Gera conclus√µes usando reasoning h√≠brido"""
        if self.current_analysis is None:
            self.perform_eda()
        
        print("üß† Gerando conclus√µes com reasoning h√≠brido...")
        
        system_prompt = """Voc√™ √© um especialista em an√°lise de dados. Com base na an√°lise completa, 
        forne√ßa conclus√µes t√©cnicas, insights pr√°ticos e recomenda√ß√µes espec√≠ficas sobre o dataset."""
        
        conclusions_prompt = f"""
        AN√ÅLISE COMPLETA DO DATASET:
        
        Dados b√°sicos:
        - {self.current_df.shape[0]:,} registros, {self.current_df.shape[1]} colunas
        - Qualidade: {sum(self.current_analysis.missing_values.values())} dados faltantes
        - Fraude: {self.current_analysis.fraud_indicators}
        
        Hist√≥rico de an√°lises:
        {self.memory.get_context()}
        
        Forne√ßa conclus√µes espec√≠ficas sobre:
        1. Qualidade e caracter√≠sticas dos dados
        2. Padr√µes de fraude identificados
        3. Insights t√©cnicos relevantes
        4. Recomenda√ß√µes pr√°ticas para uso dos dados
        5. Pr√≥ximos passos sugeridos
        """
        
        conclusions = self.reasoning.query_with_fallback(conclusions_prompt, system_prompt)
        
        self.memory.add_conversation("Conclus√µes gerais", conclusions, "conclusions")
        
        print("‚úÖ Conclus√µes geradas com reasoning h√≠brido")
        return conclusions
        
    def get_basic_info(self) -> Dict:  # <- MOVER PARA AQUI
        """Retorna informa√ß√µes b√°sicas do dataset"""
        if self.current_df is None:
            return {"error": "Nenhum dataset carregado"}
        
        df = self.current_df
        
        # Informa√ß√µes b√°sicas
        info = {
            "linhas": df.shape[0],
            "colunas": df.shape[1],
            "colunas_nomes": list(df.columns),
            "tipos_dados": {str(k): str(v) for k, v in df.dtypes.to_dict().items()},
            "dados_faltantes": {str(k): int(v) for k, v in df.isnull().sum().to_dict().items()},
            "memoria_mb": float(df.memory_usage(deep=True).sum() / 1024**2)
        }
        
        # Se tem coluna Class (fraude)
        if 'Class' in df.columns:
            fraud_count = df['Class'].sum()
            info["fraude"] = {
                "total_fraudes": int(fraud_count),
                "taxa_fraude": float((fraud_count / len(df)) * 100),
                "distribuicao": {str(k): int(v) for k, v in df['Class'].value_counts().to_dict().items()}
            }
        
        return info

# Fun√ß√£o de teste
def test_hybrid_agent():
    """Testa o agente h√≠brido"""
    print("ü§ñ Testando Agente CSV H√≠brido I2A2")
    print("=" * 50)
    
    # Tentar carregar chave OpenAI do ambiente
    openai_key = os.getenv('OPENAI_API_KEY')
    if not openai_key:
        print("üí° Para usar OpenAI, defina: export OPENAI_API_KEY=sua_chave")
        print("üí° Ou crie arquivo .env com OPENAI_API_KEY=sua_chave")
    
    agent = CSVAgentHybrid(openai_api_key=openai_key)
    
    test_file = "../creditcard.csv"
    
    if os.path.exists(test_file):
        print(f"üìÅ Testando com: {test_file}")
        success = agent.load_csv(test_file)
        
        if success:
            print("‚úÖ CSV carregado com sucesso")
            
            # Teste com reasoning h√≠brido
            response = agent.answer_question("Quais s√£o as principais caracter√≠sticas deste dataset de fraudes?")
            print(f"ü§ñ Resposta com reasoning: {response['answer'][:300]}...")
            
        else:
            print("‚ùå Erro ao carregar CSV")
    else:
        print(f"‚ùå Arquivo n√£o encontrado: {test_file}")
    
    return agent

if __name__ == "__main__":
    test_hybrid_agent()
